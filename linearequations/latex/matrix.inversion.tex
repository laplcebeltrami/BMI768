\documentclass[runningheads,a4paper]{llncs}

\usepackage{amsfonts,amssymb,amsmath}
%\setcounter{tocdepth}{3}
\usepackage{natbib}

\usepackage{wrapfig}
\usepackage{llncsdoc}
\usepackage{epsfig,psfrag}
\usepackage{latexsym}
\usepackage{longtable}
\usepackage{tikz,pgf}
\usepackage{subfig}
\usepackage{wrapfig}
%\usepackage{rotating}
%\usepackage{algorithm}
%\usepackage{algorithmic}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{color}

\newcommand{\bqn}{\begin{eqnarray}}
\newcommand{\eqn}{\end{eqnarray}}
\newcommand{\bq}{\begin{eqnarray*}}
\newcommand{\eq}{\end{eqnarray*}}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{url}
\usepackage{hyperref}
\hypersetup{colorlinks,%
citecolor=black,%
filecolor=blue,%
linkcolor=red,%
urlcolor=blue,%
pdftex}




\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\green}[1]{{\color{green} #1}}
\newcommand{\red}[1]{{\color{red} #1}}

\begin{document}



\title{Solving Large Linear Equations}
% \titlerunning{Large Matrix Inversion}
\author{Moo K. Chung
%\hspace{0.2cm}$
%\footnote[0]{Corresponding address: Moo K. Chung, Waisman Center $\#$281, 1500 Highland Ave. Madison, WI.  53705. USA. Email:{\tt mkhung@wisc.edu}. \url{http://www.stat.wisc.edu/~mchung}}, 
 }
\institute{
University of Wisconsin-Madison, USA\\
\vspace{0.3cm}
\blue{\tt mkchung@wisc.edu}
}
\authorrunning{Chung}



\maketitle


%\pagestyle{empty}

%\pagestyle{headings}
%\setcounter{page}{1}
\pagenumbering{arabic}


\begin{abstract}
In this short lecture, we explain how to solve large matrix systems \( {\bf y} = A \mathbf{b} \), where \( A \) can be on the order of hundreds of thousands. We discuss the challenges associated with large-scale computations and explore strategies such as leveraging sparse matrices, utilizing iterative solvers, applying preconditioning techniques. A practical Matlab example is provided to demonstrate these methods in action.
\end{abstract}


\section{Introduction}
Directly solving such large systems using standard matrix inversion techniques can be computationally intensive and impractical. Storing large matrices in memory can quickly exhaust available resources. For instance, a dense \( 300{,}000 \times 300{,}000 \) matrix requires approximately \( 720 \) GB of memory (assuming double-precision floating-point numbers, which occupy 8 bytes each), which most end users do not have access to. Often, \( 300{,}000 \) is the scale of numbers in brain imaging. There are approximately \( 300{,}000 \) voxels in the brain (covering both white and gray matter) at a 1mm resolution in MRI scans. Additionally, cortical surface meshes typically contain around \( 300{,}000 \) triangles in each hemisphere. Even if memory is sufficient, the computational time for operations like matrix inversion scales poorly with matrix size. Traditional algorithms for solving \( y = A \mathbf{b} \) have computational complexities in quadratic run time.


\section{Least squares estimation}

Consider linear equation 
$${\bf y} = A \mathbf{b},$$ 
where \( A \) is an \( M \times N \) matrix, potentially with \( M, N \) in the hundreds of thousands.
\( \mathbf{b} \) is a vector or matrix we need to estimate.
\( {\bf y} \) is the given data or observation. 

If \(A\) is square (\(M = N\)) and invertible, the solution is obtained as
\[
\mathbf{b} = A^{-1}\mathbf{y}.
\]
However, when \(A\) is not square or is rank-deficient, an exact inverse does not exist, and we resort to generalized solutions using the least-squares solution, which minimizes the squared residual \(\|\mathbf{y} - A\mathbf{b}\|_2^2\). The solution is given by the normal equations
\[
A^\top A \mathbf{b} = A^\top \mathbf{y},
\]
where \(A^\top\) is the transpose of \(A\). The solution is:
\[
\mathbf{b} = (A^\top A)^{-1}A^\top \mathbf{y},
\]
assuming \(A^\top A\) is invertible.





\section{Generalized Inverse }
When \(A\) is ill-conditioned or rank-deficient, \(A^\top A\) may not invertible. Then we apply Tikhonov regularization, which minimizes
\[
\|\mathbf{y} - A\mathbf{b}\|_2^2 + \lambda \|\mathbf{b}\|_2^2,
\]
where \(\lambda > 0\) is a regularization parameter. The solution is:
\[
\mathbf{b} = (A^\top A + \lambda I)^{-1}A^\top \mathbf{y}.
\]
Note the eigenvalues of $A^\top A$ are nonnegative. Adding $\lambda I$ with $\lambda > 0$ shifts all eigenvalues of $A^\top A by $\lambda$, thus  $A^\top A + \lambda I$ is invertible. 


Alternatively, the pseudo-inverse (also called the Moore-Penrose inverse) provides a generalized solution. The pseudo-inverse of a matrix \(A\), denoted as \(A^+\), is used to find the least-squares solution:
\[
\mathbf{b} = A^+ \mathbf{y}.
\]

The pseudo-inverse \(A^+\) satisfies the following properties
\[
A A^+ A = A, \quad A^+ A A^+ = A^+, \quad (A A^+)^\top = A A^+, \quad (A^+ A)^\top = A^+ A.
\]

When \(M > N\) (overdetermined system), the pseudo-inverse is given by
\[
A^+ = (A^\top A)^{-1}A^\top.
\]
When \(M < N\) (underdetermined system), the pseudo-inverse is expressed as
\[
A^+ = A^\top (A A^\top)^{-1}.
\]
These solutions minimize the squared residual \(\|\mathbf{y} - A\mathbf{b}\|_2^2\), which is equivalent to solving the normal equations:
\[
A^\top A \mathbf{b} = A^\top \mathbf{y}.
\]



The pseudo-inverse of a matrix \(A\) can be computed using its Singular Value Decomposition (SVD). For a matrix \(A\), the SVD decomposes \(A\) as 
$$A = U \Sigma V^\top\),$$ 
where \(U\) and \(V\) are orthogonal matrices, and \(\Sigma \) is a diagonal matrix containing the singular values \(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\), with \(r = \text{rank}(A)\).
The pseudo-inverse \(A^+\) is then expressed as 
$$A^+ = V \Sigma^+ U^\top,$$ 
where \(\Sigma^+\) is the pseudo-inverse of \(\Sigma\) given by
$$\Sigma^+ = \text{diag}\left(\frac{1}{\sigma_1}, \frac{1}{\sigma_2}, \ldots, \frac{1}{\sigma_r}, 0, \ldots, 0\right).$$ 
The SVD-based pseudo-inverse is computationally stable and avoids the numerical issues associated with direct inversion of \(A^\top A\) or \(A A^\top\), making it especially suitable for large or ill-conditioned matrices.


%\section{Matlab Example}




\bibliographystyle{agsm} 
\bibliography{reference.2021.12.01}

\end{document}
